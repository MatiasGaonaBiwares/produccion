{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d39238c-8e57-4089-885e-b128d8ffbf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2198: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "<ipython-input-2-922710abc5ae>:480: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_word['word']=0\n"
     ]
    }
   ],
   "source": [
    "# Archivos usados:\n",
    "# modelo:'modelV4_BETO_uncased_fine_tune.pkl'\n",
    "# dicc temas: 'Diccionario temas.xlsx'\n",
    "# stopwords: 'Stop_words_sin_tilde1.xlsx'\n",
    "\n",
    "\n",
    "#PROXIMOS PASOS:\n",
    "# Agregar al dataset de sentimiento filas de PQRS provenientes de web y correo electrónico que no sean de agentes\n",
    "# Relacionar las encuestas de PQRS a los PQRS para clasificar bien la temática\n",
    "# Pensar sobre que conjunto de datos se van a obtener las frases más frecuentes. (sobre que periodo, origen, etc.)\n",
    "# Buscar mejoras en general en frases y palabras, sobre todo con limpieza de stop words y stop sentences\n",
    "\n",
    "\n",
    "\n",
    "################################# LIBRERIAS Y CONEXION ################################################\n",
    "\n",
    "from sqlalchemy.engine import create_engine #Genera la conexión para generar queries\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import BertModel,BertTokenizer\n",
    "from torch import nn\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "def conectionOracleDB():\n",
    "    \"\"\" Configura la conexión a la bbdd proporcionada por Biwares(Nano). Devuleve la ruta de conexión como 'engine' \"\"\"\n",
    "\n",
    "    DIALECT = 'oracle'\n",
    "    SQL_DRIVER = 'cx_oracle'\n",
    "    USERNAME = 'EFECTY_VOC' #enter your username\n",
    "    PASSWORD = 'Ef3cty2021' #enter your password\n",
    "    HOST = 'insis.cl5u1tcuiscl.us-east-1.rds.amazonaws.com' #enter the oracle db host url\n",
    "    PORT = 1521 # enter the oracle port number\n",
    "    SERVICE = 'ORCL' # enter the oracle db service name\n",
    "    ENGINE_PATH_WIN_AUTH = DIALECT + '+' + SQL_DRIVER + '://' + USERNAME + ':' + PASSWORD +'@' + HOST + ':' + str(PORT) + '/?service_name=' + SERVICE\n",
    "    \n",
    "    engine = create_engine(ENGINE_PATH_WIN_AUTH)\n",
    "    \n",
    "    return engine\n",
    "\n",
    "######################################### QUERY ##############################################\n",
    "\n",
    "BT_COMENTARIOS= pd.read_sql_query(\"SELECT P.ENCUESTA, M.canal, M.MEDIO_INGRESO, I.INTERACCION_ID,C.ID ,C.TEMA_ID ,C.COMENTARIO, C.WRAPUP ,C.SUBTIPO ,C.FRASES ,C.SENTIMIENTO ,C.FECHA_PROCESO  FROM EFECTY_VOC.BT_COMENTARIOS C LEFT JOIN EFECTY_VOC.BT_INTERACCIONES I ON C.INTERACCION_ID=I.INTERACCION_ID LEFT JOIN EFECTY_VOC.LK_MEDIO_INGRESO M ON M.MEDIO_INGRESO_ID=I.MEDIO_INGRESO_ID LEFT JOIN LK_PREGUNTA P ON P.pregunta_id=I.pregunta_id fetch first 2000 row ONLY \", conectionOracleDB()) #fetch first 100 row ONLY\n",
    "# BT_COMENTARIOS_1 = pd.read_sql_query(\"SELECT P.ENCUESTA, M.canal, M.MEDIO_INGRESO, I.INTERACCION_ID,C.ID ,C.TEMA_ID ,C.COMENTARIO, C.WRAPUP ,C.SUBTIPO ,C.FRASES ,C.SENTIMIENTO ,C.FECHA_PROCESO  FROM EFECTY_VOC.BT_COMENTARIOS C LEFT JOIN EFECTY_VOC.BT_INTERACCIONES I ON C.INTERACCION_ID=I.INTERACCION_ID LEFT JOIN EFECTY_VOC.LK_MEDIO_INGRESO M ON M.MEDIO_INGRESO_ID=I.MEDIO_INGRESO_ID LEFT JOIN LK_PREGUNTA P ON P.pregunta_id=I.pregunta_id  where canal='PQRS' fetch first 100 row ONLY\", conectionOracleDB())\n",
    "# BT_COMENTARIOS_2 = pd.read_sql_query(\"SELECT P.ENCUESTA, M.canal, M.MEDIO_INGRESO, I.INTERACCION_ID,C.ID ,C.TEMA_ID ,C.COMENTARIO ,C.WRAPUP ,C.SUBTIPO ,C.FRASES ,C.SENTIMIENTO ,C.FECHA_PROCESO  FROM EFECTY_VOC.BT_COMENTARIOS C LEFT JOIN EFECTY_VOC.BT_INTERACCIONES I ON C.INTERACCION_ID=I.INTERACCION_ID LEFT JOIN EFECTY_VOC.LK_MEDIO_INGRESO M ON M.MEDIO_INGRESO_ID=I.MEDIO_INGRESO_ID LEFT JOIN LK_PREGUNTA P ON P.pregunta_id=I.pregunta_id  where canal='RRSS' fetch first 100 row ONLY\", conectionOracleDB())\n",
    "# BT_COMENTARIOS_3 = pd.read_sql_query(\"SELECT P.ENCUESTA, M.canal, M.MEDIO_INGRESO, I.INTERACCION_ID,C.ID ,C.TEMA_ID ,C.COMENTARIO ,C.WRAPUP ,C.SUBTIPO ,C.FRASES ,C.SENTIMIENTO ,C.FECHA_PROCESO  FROM EFECTY_VOC.BT_COMENTARIOS C LEFT JOIN EFECTY_VOC.BT_INTERACCIONES I ON C.INTERACCION_ID=I.INTERACCION_ID LEFT JOIN EFECTY_VOC.LK_MEDIO_INGRESO M ON M.MEDIO_INGRESO_ID=I.MEDIO_INGRESO_ID LEFT JOIN LK_PREGUNTA P ON P.pregunta_id=I.pregunta_id  where canal='ENCUESTA' fetch first 100 row ONLY\", conectionOracleDB())\n",
    "# BT_COMENTARIOS=pd.concat([BT_COMENTARIOS_3,BT_COMENTARIOS_2,BT_COMENTARIOS_1])\n",
    "# BT_COMENTARIOS.reset_index(inplace=True)\n",
    "\n",
    "#################################################### TRABAJO SOBRE EL TEXTO #######################\n",
    "\n",
    "#El comentario tal cual lo descargo, quedará con la marca \"comentario inicial\"\n",
    "BT_COMENTARIOS['comentario_inicial']=BT_COMENTARIOS['comentario']\n",
    "\n",
    "#Lo paso a string y lo hago todo lower\n",
    "BT_COMENTARIOS['comentario']=BT_COMENTARIOS['comentario'].astype(str)\n",
    "BT_COMENTARIOS['comentario']=BT_COMENTARIOS['comentario'].apply(lambda x:x.lower())\n",
    "\n",
    "#Modifico algunas cuestiones que afectan sobre todo a los PQRS: quito los /n, y los numeros a un mismo formato\n",
    "BT_COMENTARIOS['comentario']=BT_COMENTARIOS['comentario'].apply(lambda x: re.sub(r'[\\n]{1,4}',' ',x)) #Mas que nada para las PQRS que tienen muchas lineas vacias, donde hay mucho \\n textual\n",
    "BT_COMENTARIOS['comentario']=BT_COMENTARIOS['comentario'].apply(lambda x: re.sub(r\"\"\"[' ']{1,4}\"\"\",' ',x)) #CONTINUACIÓN DE LO DE ARRIBA\n",
    "BT_COMENTARIOS['comentario']=BT_COMENTARIOS['comentario'].apply(lambda x: re.sub(r\"\"\"[0-9]{1,10}\"\"\",'número',x))\n",
    "\n",
    "#Limpio comentarios de RRSS (no se tiene en cuenta medio de ingreso messenger)\n",
    "\n",
    "BT_COMENTARIOS['comentarioRRSS']=BT_COMENTARIOS['comentario'].apply(lambda x: re.findall(\"Cliente:\\**(.*)\\n\\n\",x) if re.match('.*\\d\\d/\\d\\d/\\d\\d\\d\\d.*',x) else x)\n",
    "BT_COMENTARIOS['comentarioRRSS']=BT_COMENTARIOS['comentarioRRSS'].apply(lambda x: re.sub('\\?','zzz',str(x)))\n",
    "BT_COMENTARIOS['comentarioRRSS']=BT_COMENTARIOS['comentarioRRSS'].apply(lambda x: re.sub('\\.','bbb',str(x)))\n",
    "BT_COMENTARIOS['comentarioRRSS']=BT_COMENTARIOS['comentarioRRSS'].apply(lambda x: re.sub(',','qwqz',str(x)))\n",
    "BT_COMENTARIOS['comentarioRRSS']=BT_COMENTARIOS['comentarioRRSS'].apply(lambda x: re.sub('[\\W]',' ',str(x)))\n",
    "BT_COMENTARIOS['comentarioRRSS']=BT_COMENTARIOS['comentarioRRSS'].apply(lambda x: re.sub('qwqz',',',str(x)))\n",
    "BT_COMENTARIOS['comentarioRRSS']=BT_COMENTARIOS['comentarioRRSS'].apply(lambda x: re.sub('bbb','.',str(x)))\n",
    "BT_COMENTARIOS['comentarioRRSS']=BT_COMENTARIOS['comentarioRRSS'].apply(lambda x: re.sub('zzz','?',str(x)))\n",
    "BT_COMENTARIOS['comentarioRRSS']=BT_COMENTARIOS['comentarioRRSS'].apply(lambda x: re.sub('\\s\\s',' ',str(x)))\n",
    "BT_COMENTARIOS['comentarioRRSS']=BT_COMENTARIOS['comentarioRRSS'].apply(lambda x: re.sub('Imagen .*? [0-9-a-z]{10,30}','',str(x)))\n",
    "index_rrss=BT_COMENTARIOS[(BT_COMENTARIOS['canal']=='RRSS')&(BT_COMENTARIOS['medio_ingreso']!='MESSENGER')].index\n",
    "BT_COMENTARIOS.loc[index_rrss,'comentario']=BT_COMENTARIOS['comentarioRRSS']\n",
    "BT_COMENTARIOS.drop(columns='comentarioRRSS',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "############################# GENERACIÓN DEL SENTIMIENTO ##############################################\n",
    "\n",
    "\n",
    "#cargo el tokenizador de beto\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "\n",
    "#donde voy a trabajar\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Vuelvo a definir la estructura del modelo donde voy a trabajar ( es la misma que en el entrenamiento)\n",
    "class BERTSentimentClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(BERTSentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(\"pytorch/\")\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, cls_output = self.bert(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attention_mask,\n",
    "        return_dict=False\n",
    "    )\n",
    "    drop_output = self.drop(cls_output)\n",
    "    output = self.linear(drop_output)\n",
    "    return output\n",
    "\n",
    "#Cargo el modelo\n",
    "model = torch.load('modelV4_BETO_uncased_fine_tune.pkl', map_location=torch.device('cpu'))\n",
    "\n",
    "#Defino el proceso del modelo para analizar el texto\n",
    "def classifySentiment(review_text):\n",
    "  encoding_review = tokenizer.encode_plus(\n",
    "      review_text,\n",
    "      truncation = True,\n",
    "      add_special_tokens = True,\n",
    "      return_token_type_ids = False,\n",
    "      # padding=True,\n",
    "      pad_to_max_length = True,\n",
    "      return_attention_mask = True,\n",
    "      return_tensors = 'pt'\n",
    "      )\n",
    "  \n",
    "  input_ids = encoding_review['input_ids'].to(device)\n",
    "  attention_mask = encoding_review['attention_mask'].to(device)\n",
    "  output = model(input_ids, attention_mask)\n",
    "  _, prediction = torch.max(output, dim=1) #El output es un vector de NCLASSES dimensiones (3). Aca, se va a quedar con el valor mas alto, quien será el elegido \n",
    "#   return prediction.tolist()[0],output.tolist()[0][0],output.tolist()[0][1],output.tolist()[0][2]\n",
    "  return prediction.tolist()[0]\n",
    "\n",
    "#Aplico el modelo\n",
    "BT_COMENTARIOS['sentimientoV4']=BT_COMENTARIOS['comentario'].apply(lambda x:classifySentiment(x))\n",
    "\n",
    "#Trabajo sobre los resultados\n",
    "BT_COMENTARIOS['sentimiento']=BT_COMENTARIOS['sentimientoV4'].apply(lambda x: 0.5 if x==1 else x)\n",
    "BT_COMENTARIOS['sentimiento']=BT_COMENTARIOS['sentimiento'].apply(lambda x: 1 if x==2 else x)\n",
    "BT_COMENTARIOS.drop(columns=['sentimientoV4'],inplace=True)\n",
    "\n",
    "# Un grupo de comentarios tendrá valor Null en sentimiento porque no aplican:\n",
    "index_s=BT_COMENTARIOS[((BT_COMENTARIOS['canal']=='PQRS')&\n",
    "                              (BT_COMENTARIOS['medio_ingreso']!='WEB')&\n",
    "                              (BT_COMENTARIOS['medio_ingreso']!='CORREO ELECTRONICO'))|\n",
    "               ((BT_COMENTARIOS['canal']=='RRSS')&\n",
    "                              (BT_COMENTARIOS['medio_ingreso']=='MESSENGER'))].index\n",
    "BT_COMENTARIOS.loc[index_s,'sentimiento']=None\n",
    "\n",
    "\n",
    "####################################### GENERACION DE LOS TEMAS ###########################################\n",
    "\n",
    "#Extracción del diccionario\n",
    "DicTemas = pd.read_excel('Diccionario temas.xlsx')\n",
    "\n",
    "#Genero los diccionarios según el origen\n",
    "Dic_Temas=dict(list(zip(DicTemas['SUBTIPO/WRAPUP/ENCUESTA'],DicTemas['TEMÁTICA'])))\n",
    "Dic_Temas_subtipo=dict(list(zip(DicTemas['SUBTIPO/WRAPUP/ENCUESTA'][DicTemas['ORIGEN']=='Subtipo-PQRS'],DicTemas['TEMÁTICA'][DicTemas['ORIGEN']=='Subtipo-PQRS'])))\n",
    "Dic_Temas_wrapups=dict(list(zip(DicTemas['SUBTIPO/WRAPUP/ENCUESTA'][DicTemas['ORIGEN']=='wrapups'],DicTemas['TEMÁTICA'][DicTemas['ORIGEN']=='wrapups'])))\n",
    "Dic_Temas_enc=dict(list(zip(DicTemas['SUBTIPO/WRAPUP/ENCUESTA'][DicTemas['ORIGEN']=='Encuestas'],DicTemas['TEMÁTICA'][DicTemas['ORIGEN']=='Encuestas'])))\n",
    "\n",
    "BT_COMENTARIOS.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "#Paso a formato string las columnas donde voy a aplicar el diccionario\n",
    "BT_COMENTARIOS['subtipo']=BT_COMENTARIOS['subtipo'].astype(str)\n",
    "BT_COMENTARIOS['wrapup']=BT_COMENTARIOS['wrapup'].astype(str)\n",
    "BT_COMENTARIOS['encuesta']=BT_COMENTARIOS['encuesta'].astype(str)\n",
    "\n",
    "#Extracción de index para según origen\n",
    "index_sub=BT_COMENTARIOS[BT_COMENTARIOS['subtipo']!='None'].index\n",
    "index_rrss=BT_COMENTARIOS[BT_COMENTARIOS['wrapup']!='None'].index\n",
    "index_enc=BT_COMENTARIOS[BT_COMENTARIOS['encuesta']!='None'].index\n",
    "\n",
    "#Alpico los diccionarios según los index\n",
    "BT_COMENTARIOS.loc[index_sub,'tema_id']=BT_COMENTARIOS['subtipo'].apply(lambda x: Dic_Temas_subtipo.get(x) if  Dic_Temas_subtipo.get(x) is not None else x)\n",
    "BT_COMENTARIOS.loc[index_rrss,'tema_id']=BT_COMENTARIOS['wrapup'].apply(lambda x: Dic_Temas_wrapups.get(x) if  Dic_Temas_wrapups.get(x) is not None else x)\n",
    "BT_COMENTARIOS.loc[index_enc,'tema_id']=BT_COMENTARIOS['encuesta'].apply(lambda x: Dic_Temas_enc.get(x) if  Dic_Temas_enc.get(x) is not None else x)\n",
    "\n",
    "\n",
    "############################# EXTRACCION DE FRASES ######################################\n",
    "\n",
    "\n",
    "# #Trabajo sobre un subdataframe para extracción de frases. No me interesa extraer frases de comentarios que no son del cliente\n",
    "BT_COMENTARIOS_FRASES=BT_COMENTARIOS.copy()\n",
    "\n",
    "BT_COMENTARIOS_FRASES=BT_COMENTARIOS[\n",
    "                              ((BT_COMENTARIOS['canal']=='PQRS')&\n",
    "                                  ((BT_COMENTARIOS['medio_ingreso']=='WEB')|\n",
    "                                  (BT_COMENTARIOS['medio_ingreso']=='CORREO ELECTRONICO')))|\n",
    "                              ((BT_COMENTARIOS['canal']=='RRSS')&\n",
    "                                  (BT_COMENTARIOS['medio_ingreso']!='MESSENGER'))|\n",
    "                              (BT_COMENTARIOS['canal']=='ENCUESTA')]\n",
    "\n",
    "\n",
    "#Genero 3 bases. Se van a extraer las frases mas comunes para cada grupo\n",
    "BT_COMENTARIOS_FRASES_RRSS=BT_COMENTARIOS_FRASES[BT_COMENTARIOS_FRASES['canal']=='RRSS']\n",
    "BT_COMENTARIOS_FRASES_ENCUESTA=BT_COMENTARIOS_FRASES[BT_COMENTARIOS_FRASES['canal']=='ENCUESTA']\n",
    "BT_COMENTARIOS_FRASES_PQRS=BT_COMENTARIOS_FRASES[BT_COMENTARIOS_FRASES['canal']=='PQRS']\n",
    "\n",
    "\n",
    "#Importo stopwords\n",
    "stop_words = (pd.read_excel('Stop_words_sin_tilde.xlsx')) \n",
    "stop_words=list(stop_words['stopwords']) \n",
    "\n",
    "#Defino las frases que no me aportan valor\n",
    "Elim_Fras_PQRS=['por favor','gracias','quedamos atentos','feliz','cordialmente','quedo atento',' c ',' com ',' co ',\n",
    "'url','post','none','numero','hola','te informamos','efecty bienvenido','cliente htt','api',\n",
    "'imagen enviada','puresocial','email','tratamiento de datos','acepta politica','twitter','pm','am',\n",
    "'siguientes datos','nombres','por favor confirma','htt','www','facebook','efectyoficial','twitterefectyoficial','efecty'\n",
    " 'cliente buenas taredes','hotmail','gmail','usuario se comunica','usuaria se comunica','confronta exitosa','usuario solicita información',\n",
    " 'usuaria solicita información','se informa que','se le indica correo','se realiza','se solicita','se genera certificación de pagos',\n",
    " 'se envía la solicitud al correo','por parte del usuariose envía','se genera certificación de pagos','del proyecto','se informa',\n",
    " 'se brinda','usuaria','solicita información','se informa que','usuario solicita','usuaria solicita','retroalimentacion','se toma solicitud',\n",
    " 'solicita','usuaria se comunica para manifestar inconformidad','usuario se comunica','se indica','se informa','cédula','celular','solicitud',\n",
    " 'cajero:','punto:','correo:','errada:','correc:','radicado:','nombre:','cc:','correo:','telefono:','direccion:','radicado:','os:',\n",
    " 'ps','usuario solicita','pago realizado','buenas tardes','datos personales','respuesta al correo electronico','correo electronico se indica','usuaria solicita',\n",
    " 'adjunta pantallazo','al correo sac','se lee articulo','se adjunta','pantallazo de correo','confronta no exitoso','usuario autoriza','buenas tardes',\n",
    " 'cordial saludo','correo electronico','dias habiles','buenos dias','respuesta al correo','autoriza respuesta','el dia de hoy','tiempo de gestion de','quedo atenta','muchas gracias','articulo de la ley','autoriza tiempo',\n",
    " 'ley','buen dia','pronta respuesta','cedula','numero de referencia','hasta la fecha','el nombre del titular']\n",
    "\n",
    "\n",
    "Elim_Fras_ENCUESTA=['por favor','gracias','quedamos atentos','feliz','cordialmente','quedo atento',' c ',' com ',' co ',\n",
    "'url','post','none','numero','hola','te informamos','efecty bienvenido','cliente htt','api',\n",
    "'imagen enviada','puresocial','email','tratamiento de datos','acepta politica','twitter','pm','am',\n",
    "'siguientes datos','nombres','por favor confirma','htt','www','facebook','efectyoficial','twitterefectyoficial','efecty'\n",
    " 'cliente buenas taredes','hotmail','gmail','usuario se comunica','usuaria se comunica','confronta exitosa','usuario solicita información',\n",
    " 'usuaria solicita información','se informa que','se le indica correo','se realiza','se solicita','se genera certificación de pagos',\n",
    " 'se envía la solicitud al correo','por parte del usuariose envía','se genera certificación de pagos','del proyecto','se informa',\n",
    " 'se brinda','usuaria','solicita información','se informa que','usuario solicita','usuaria solicita','retroalimentacion','se toma solicitud',\n",
    " 'solicita','usuaria se comunica para manifestar inconformidad','usuario se comunica','se indica','se informa','cédula','celular','solicitud',\n",
    " 'cajero:','punto:','correo:','errada:','correc:','radicado:','nombre:','cc:','correo:','telefono:','direccion:','radicado:','os:',\n",
    " 'ps','usuario solicita','pago realizado','buenas tardes','datos personales','respuesta al correo electronico','correo electronico se indica','usuaria solicita',\n",
    " 'adjunta pantallazo','al correo sac','se lee articulo','se adjunta','pantallazo de correo','confronta no exitoso','usuario autoriza','buenas tardes',\n",
    " 'cordial saludo','correo electronico','dias habiles','buenos dias','respuesta al correo','autoriza respuesta','el dia de hoy','tiempo de gestion de','quedo atenta','muchas gracias','articulo de la ley','autoriza tiempo',\n",
    " 'ley','buen dia','pronta respuesta','cedula','numero de referencia','hasta la fecha','el nombre del titular']\n",
    "\n",
    "Elim_Fras_RRSS=['por favor','gracias','quedamos atentos','feliz','cordialmente','quedo atento',' c ',' com ',' co ',\n",
    "'url','post','none','numero','hola','te informamos','efecty bienvenido','cliente htt','api',\n",
    "'imagen enviada','puresocial','email','tratamiento de datos','acepta politica','twitter','pm','am',\n",
    "'siguientes datos','nombres','por favor confirma','htt','www','facebook','efectyoficial','twitterefectyoficial','efecty'\n",
    " 'cliente buenas taredes','hotmail','gmail','usuario se comunica','usuaria se comunica','confronta exitosa','usuario solicita información',\n",
    " 'usuaria solicita información','se informa que','se le indica correo','se realiza','se solicita','se genera certificación de pagos',\n",
    " 'se envía la solicitud al correo','por parte del usuariose envía','se genera certificación de pagos','del proyecto','se informa',\n",
    " 'se brinda','usuaria','solicita información','se informa que','usuario solicita','usuaria solicita','retroalimentacion','se toma solicitud',\n",
    " 'solicita','usuaria se comunica para manifestar inconformidad','usuario se comunica','se indica','se informa','cédula','celular','solicitud',\n",
    " 'cajero:','punto:','correo:','errada:','correc:','radicado:','nombre:','cc:','correo:','telefono:','direccion:','radicado:','os:',\n",
    " 'ps','usuario solicita','pago realizado','buenas tardes','datos personales','respuesta al correo electronico','correo electronico se indica','usuaria solicita',\n",
    " 'adjunta pantallazo','al correo sac','se lee articulo','se adjunta','pantallazo de correo','confronta no exitoso','usuario autoriza','buenas tardes',\n",
    " 'cordial saludo','correo electronico','dias habiles','buenos dias','respuesta al correo','autoriza respuesta','el dia de hoy','tiempo de gestion de','quedo atenta','muchas gracias','articulo de la ley','autoriza tiempo',\n",
    " 'ley','buen dia','pronta respuesta','cedula','numero de referencia','hasta la fecha','el nombre del titular']\n",
    "\n",
    "\n",
    "\n",
    "def limp_comentarios(df,col1):\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub('[\\W]',' ',str(x))) #elimino todo lo que no es numero o letra\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub('\\s\\s',' ',str(x))) #elimino dbole espacio generado por lo de arriba\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub('á','a',str(x)))\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub('é','e',str(x)))\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub('í','i',str(x)))\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub('ó','o',str(x)))\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub('ú','u',str(x)))\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub(r\"\\wW]\",\" \",x))\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub(r\"\"\"[\\n\\t]\"\"\",\" \",x))\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub(\"\"\"[-*\\_¡!@#$:).;–,¿?&'_>/]{1,4}\"\"\",\"\",x))\n",
    "    df[col1]=df[col1].apply(lambda x: re.sub(r\"\"\"[0-9]\"\"\",\"\",x))\n",
    "\n",
    "limp_comentarios(BT_COMENTARIOS_FRASES_RRSS,'comentario')\n",
    "limp_comentarios(BT_COMENTARIOS_FRASES_PQRS,'comentario')\n",
    "limp_comentarios(BT_COMENTARIOS_FRASES_ENCUESTA,'comentario')\n",
    "\n",
    "#Elimino las frases que no aportan valor\n",
    "def Elimifrases(fras_elim,df,column):\n",
    "    for f in fras_elim:   \n",
    "        df[column]=df[column].apply(lambda x: re.sub(f,'',str(x)))\n",
    "\n",
    "Elimifrases(Elim_Fras_RRSS,BT_COMENTARIOS_FRASES_RRSS,'comentario')\n",
    "Elimifrases(Elim_Fras_PQRS,BT_COMENTARIOS_FRASES_PQRS,'comentario')\n",
    "Elimifrases(Elim_Fras_ENCUESTA,BT_COMENTARIOS_FRASES_ENCUESTA,'comentario')\n",
    "\n",
    "\n",
    "def tokenizar(df,column):\n",
    "    pa_token=re.sub(\"\"\"[,']\"\"\",'',str(list(df[column])))\n",
    "    tokenized_word=word_tokenize(pa_token)\n",
    "    return tokenized_word\n",
    "\n",
    "tokRRSS=tokenizar(BT_COMENTARIOS_FRASES_RRSS,'comentario')\n",
    "tokPQRS=tokenizar(BT_COMENTARIOS_FRASES_PQRS,'comentario')\n",
    "tokENCUESTA=tokenizar(BT_COMENTARIOS_FRASES_ENCUESTA,'comentario')\n",
    "\n",
    "\n",
    "##desde aca, revisar\n",
    "\n",
    "def gen_frases(df,qty_words_ngram,max_stopwords,tok,qty_most_common,col):\n",
    "    global Top_words\n",
    "    Top_words=[]\n",
    "    Encuestas2=pd.DataFrame()\n",
    "    ongrams=ngrams(tok,qty_words_ngram)\n",
    "    ongrams=list(zip(ongrams))\n",
    "    ongrams=pd.DataFrame(ongrams)\n",
    "    ongrams['r']=0\n",
    "    for i in range(0,qty_words_ngram):\n",
    "        ongrams['val'+str(i)]=ongrams[0].apply(lambda x: x[i])\n",
    "        ongrams['w'+str(i)]=0\n",
    "        index_on_val=ongrams[ongrams['val'+str(i)].isin(stop_words)].index\n",
    "        ongrams.loc[index_on_val,'w'+str(i)]=1\n",
    "        ongrams['r']=ongrams['r']+ongrams['w'+str(i)]\n",
    "\n",
    "    ongrams=ongrams[(ongrams['r']<=max_stopwords)&(ongrams['w'+str(qty_words_ngram-1)]==0)] #La ultima condicion es para que no termine con un stopword\n",
    "    ongrams_=[]\n",
    "    ongrams.reset_index(inplace=True)\n",
    "\n",
    "    for i in range(len(ongrams[0])):\n",
    "        ongrams_.append(re.sub(\"\"\"[¡!@)#($:.;,¿?&'_]\"\"\",\"\",str(ongrams[0][i]))) #Agregar nan\n",
    "\n",
    "\n",
    "    fdist = FreqDist((ongrams_))\n",
    "\n",
    "    df['frases']=''\n",
    "    for i in range(0,qty_most_common-1):\n",
    "        index_frases=df[(df[col].str.contains(str(fdist.most_common(qty_most_common)[i][0])))&(df['frases']=='')].index\n",
    "        index_frases_duplicadas=df[(df[col].str.contains(str(fdist.most_common(qty_most_common)[i][0])))&(df['frases']!='')].index\n",
    "#Si, veo que esta en el top, pero que no está en las frases self.dataframe['Descripción'], es porque son 2 palabras que entre medio tenian una frase que \n",
    "# se eliminó y queda un espacio grande entre medio que te lo considera dentro del top, pero cuando lo busca no lo encuentra,\n",
    "# porque no existe tal cual, sino con un espacio de por medio y una frase sin sentido.\n",
    "        df.loc[index_frases,'frases']=fdist.most_common(qty_most_common)[i][0]\n",
    "        Top_words.append(fdist.most_common(qty_most_common)[i][0])\n",
    "        aux2=df.loc[index_frases_duplicadas,:]\n",
    "        aux2.loc[:,'frases']=fdist.most_common(qty_most_common)[i][0]\n",
    "        Encuestas2=pd.concat([aux2,Encuestas2])\n",
    "    try:\n",
    "        Encuestas2.drop(columns=['level_0'])\n",
    "        Encuestas2.reset_index(inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "#     Encuestas2.drop(columns=['level_0'])\n",
    "    df=pd.concat([df,Encuestas2])\n",
    "    try:\n",
    "        Encuestas2.drop(columns=['level_0'])\n",
    "        Encuestas2.reset_index(inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    Encuestas_con_frases=df[df['frases']!='']\n",
    "#     self.dataframe['frases']=self.dataframe['frases'].apply(lambda x: x.upper())\n",
    "#     Top_words=fdist.most_common(self.qty_most_common)\n",
    "\n",
    "    return df,Top_words\n",
    "\n",
    "\n",
    "#Extracción de frases segun cantidad de palabras y stopwords + generación de una columna en el Df\n",
    "#Guardo las top frases en top_2_palabras,\n",
    "gen_frases(BT_COMENTARIOS_FRASES_ENCUESTA,2,0,tokENCUESTA,20,'comentario')\n",
    "R_frases2=BT_COMENTARIOS_FRASES_ENCUESTA[BT_COMENTARIOS_FRASES_ENCUESTA['frases']!='']\n",
    "top_2_palabras=pd.DataFrame(Top_words)\n",
    "\n",
    "gen_frases(BT_COMENTARIOS_FRASES_ENCUESTA,3,1,tokENCUESTA,20,'comentario')\n",
    "R_frases3=BT_COMENTARIOS_FRASES_ENCUESTA[BT_COMENTARIOS_FRASES_ENCUESTA['frases']!='']\n",
    "top_3_palabras=pd.DataFrame(Top_words)\n",
    "\n",
    "gen_frases(BT_COMENTARIOS_FRASES_ENCUESTA,4,2,tokENCUESTA,20,'comentario')\n",
    "R_frases4=BT_COMENTARIOS_FRASES_ENCUESTA[BT_COMENTARIOS_FRASES_ENCUESTA['frases']!='']\n",
    "top_4_palabras=pd.DataFrame(Top_words)\n",
    "\n",
    "\n",
    "#Concatener los resultados\n",
    "Resultado=pd.concat([BT_COMENTARIOS_FRASES_ENCUESTA,R_frases2])\n",
    "Resultado=pd.concat([Resultado,R_frases3])\n",
    "Resultado=pd.concat([Resultado,R_frases4])\n",
    "try:\n",
    "    Resultado.reset_index(inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "## y aca vendria la limpieza de abajo, que falta trabajar. Por ahora,\n",
    "# solo esta aplicado a Resultados, que solo esta la base de ENCUESTAS. HABRIA Q REPETIR LO MISMO A PQRS Y RRSS\n",
    "\n",
    "\n",
    "eliminar=[]\n",
    "index_eliminar_2=[]\n",
    "index_eliminar_3=[]\n",
    "index_eliminar_4=[]\n",
    "top_2_palabras_n=list(top_2_palabras[0])\n",
    "top_3_palabras_n=list(top_3_palabras[0])\n",
    "top_4_palabras_n=list(top_4_palabras[0])\n",
    "re.findall('[\\w]{1,15}',top_4_palabras[0][6])\n",
    "\n",
    "for j in range(0,20-1): \n",
    "    #Pensado para qty_most_common(30) palabras mas comunes de 4 palabras. Compara contra las de 4 y las de 3\n",
    "    qs=re.findall('[\\w]{1,15}',top_4_palabras[0][j])\n",
    "    # 4 palabras solo va a ir contra 3\n",
    "    \n",
    "    for i in range(0,len(qs)-1):\n",
    "                    for p1 in range(0,20-1):\n",
    "#Mira si hay 2 palabras del top4 repetidas dentro de los 4grams, si es asi lo elimina\n",
    "                        if(((qs[i]+' '+qs[i+1]) in top_4_palabras[0][p1])&(p1!=j)):\n",
    "                            eliminar.append(top_4_palabras[0][p1])\n",
    "                            top_4_palabras_n[p1]=(qs[i]+' '+qs[i+1])\n",
    "                            index_eliminar_4.append(p1)\n",
    "                        if((qs[i]+' '+qs[i+1]) in top_3_palabras[0][p1]):\n",
    "                            eliminar.append(top_3_palabras[0][p1])\n",
    "                            top_2_palabras_n[p1]=(qs[i]+' '+qs[i+1])\n",
    "                            index_eliminar_3.append(p1)\n",
    "                            \n",
    "#Mira si hay 2 palabras segúidas del top4 repetidas dentro de los 3grams, si es asi lo elimina\n",
    "                        if((qs[i]+' '+qs[i+1]) in top_3_palabras[0][p1]):\n",
    "                            eliminar.append(top_3_palabras[0][p1])\n",
    "                            top_3_palabras_n[p1]=(qs[i]+' '+qs[i+1])\n",
    "                            index_eliminar_3.append(p1)\n",
    "\n",
    "#Mira si hay 2 palabras del top4 segúidas repetidas dentro de los 2grams, si es asi lo elimina\n",
    "                        if((qs[i]+' '+qs[i+1]) in top_2_palabras[0][p1]):\n",
    "                            eliminar.append(top_2_palabras[0][p1])\n",
    "                            index_eliminar_2.append(p1)\n",
    "                            top_2_palabras_n[p1]=(qs[i]+' '+qs[i+1])\n",
    "                    \n",
    "    qs3=re.findall('[\\w]{1,15}',top_3_palabras[0][j])\n",
    "    # 3 palabras solo va a ir contra 2\n",
    "    for i in range(0,len(qs3)-1):\n",
    "        for p1 in range(0,20-1):\n",
    "            #Mira si hay 2 palabras del top3 repetidas en top 3, si es asi lo elimina\n",
    "            if(((qs3[i]+' '+qs3[i+1]) in top_3_palabras[0][p1])&(p1!=j)):\n",
    "                eliminar.append(top_3_palabras[0][p1])\n",
    "                index_eliminar_3.append(p1)\n",
    "                top_3_palabras_n[p1]=(qs3[i]+' '+qs3[i+1])\n",
    "\n",
    " #Mira si hay 2 palabras del top3 en top2, si es asi lo elimina\n",
    "            if((qs3[i]+' '+qs3[i+1]) in top_2_palabras[0][p1]):\n",
    "                eliminar.append(top_2_palabras[0][p1])\n",
    "                index_eliminar_2.append(p1)\n",
    "                top_2_palabras_n[p1]=(qs3[i]+' '+qs3[i+1])\n",
    "last_frases=top_2_palabras_n+top_3_palabras_n+top_4_palabras_n\n",
    " \n",
    "    \n",
    "#Genero lista de reducción de frases que contienen mismas palabras a su unidad mas pequeña\n",
    "# last_frases=frases.LimpiezaFrases(top_2_palabras,top_3_palabras,top_4_palabras)\n",
    "Resultado.drop_duplicates(inplace=True)\n",
    "\n",
    "#Modifico las frases con excedentes por las mas pequeñas\n",
    "for b in range(0,len(last_frases)):\n",
    "    index_a_modificar=Resultado[Resultado['frases'].str.contains(last_frases[b])].index\n",
    "    Resultado.loc[index_a_modificar,'frases']=last_frases[b] \n",
    "\n",
    "\n",
    "Resultado.drop_duplicates(inplace=True)\n",
    "\n",
    "#Modifico las frases con excedentes por las mas pequeñas\n",
    "for b in range(0,len(last_frases)):\n",
    "    index_a_modificar=Resultado[Resultado['frases'].str.contains(last_frases[b])].index\n",
    "    Resultado.loc[index_a_modificar,'frases']=last_frases[b] \n",
    "\n",
    "\n",
    "Resultado.drop_duplicates(inplace=True)\n",
    "# Resultado[Resultado['frases']!=''].groupby('frases').count().sort_values(by='index',ascending=False).head(50)[['index','id']]\n",
    "\n",
    "############################# EXTRACCION DE PALABRA ######################################\n",
    "\n",
    "#traigo todos los tokens ya generados para frases\n",
    "tokk=tokRRSS+tokPQRS+tokENCUESTA\n",
    "d=pd.DataFrame(tokk)\n",
    "\n",
    "#Eliminto todos los que sean stopwords\n",
    "index_sw=d[d[0].isin(stop_words)].index\n",
    "for i in index_sw:\n",
    "    tokk[i]='-'\n",
    "\n",
    "dic_word=FreqDist(tokk)\n",
    "del dic_word['-'] \n",
    "\n",
    "#Selecciono las primeras 70 palabras\n",
    "lis_max=list(dic_word.keys())[0:70]\n",
    "\n",
    "#Genero una tabla para asociar los comentarios a las words\n",
    "df_word=BT_COMENTARIOS[['id','comentario']]\n",
    "df_word['word']=0\n",
    "\n",
    "\n",
    "df_word_last=df_word.copy()\n",
    "\n",
    "#Asocio las words a los comentarios correspondientes\n",
    "for i in range(0,len(lis_max)-1):\n",
    "    try:\n",
    "        index_w=df_word[df_word['comentario'].str.contains(lis_max[i])&(df_word['word']==0)].index\n",
    "        aux = df_word.copy()\n",
    "        aux.loc[index_w,'word']=lis_max[i]\n",
    "        aux=aux[(aux['word']==lis_max[i])]\n",
    "        df_word_last=pd.concat([aux,df_word_last])\n",
    "    except:\n",
    "        pass\n",
    "df_word_last.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "######################## CARGA DE DATAFRAMES EN ORACLE #########################\n",
    "\n",
    "#Ultima limipeza\n",
    "BT_COMENTARIOS['comentario']=BT_COMENTARIOS['comentario_inicial']\n",
    "df_bt_comentarios=BT_COMENTARIOS[['id','interaccion_id','tema_id','comentario','wrapup','subtipo','sentimiento','fecha_proceso']]\n",
    "\n",
    "df_temas=DicTemas[['SUBTIPO/WRAPUP/ENCUESTA','TEMÁTICA','sentimiento temática']]\n",
    "dic_cort=df_temas.groupby(['TEMÁTICA','sentimiento temática']).count()\n",
    "dic_cort.reset_index(inplace=True)\n",
    "dic_cort=dic_cort[['TEMÁTICA','sentimiento temática']]\n",
    "\n",
    "df_word_lastv=df_word_last[['id','word']]\n",
    "\n",
    "\n",
    "\n",
    "#Carga de las tablas\n",
    "\n",
    "#Tabla BT_COMENTARIOS con sentimientos y temas asignados\n",
    "# df_bt_comentarios.to_sql(\"BT_COMENTARIOS\", engine, schema='EFECTY_VOC', if_exists='replace',  index=False)\n",
    "\n",
    "# Tabla que con los temas y sus sentimientos\n",
    "# dic_cort.to_sql(\"LK_TEMAS\", engine, schema='EFECTY_VOC', if_exists='replace',  index=True)\n",
    "\n",
    "#Tabla con las key words\n",
    "# df_word_lastv.to_sql(\"WORD_CLOUD\", engine, schema='EFECTY_VOC', if_exists='replace',  index=False)\n",
    "\n",
    "# Tabla con las key sentences\n",
    "\n",
    "# df_sentence.to_sql(\"SENTENCE_CLOUD\", engine, schema='EFECTY_VOC', if_exists='replace',  index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
